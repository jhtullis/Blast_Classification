{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba26fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from warnings import filterwarnings\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2922f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter warnings from numpy that show up occasionally during cv, should not affect performance\n",
    "filterwarnings(\"ignore\", message=\"invalid value encountered in cast\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27cb34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters\n",
    "# random seed\n",
    "# allows for reproducibility\n",
    "# default is 42, which is used in publication\n",
    "random_seed = 42\n",
    "\n",
    "# number of iterations of random search to use to optimize each model's hyperparameters in validation\n",
    "# increasing will improve results but increase computation time\n",
    "# requires under 3 minutes for every 20 iterations with the following system specs:\n",
    "# Processor\tIntel(R) Core(TM) i7-8650U CPU @ 1.90GHz, 2112 Mhz, 4 Core(s), 8 Logical Processor(s)\n",
    "# Installed Physical Memory (RAM)\t16.0 GB\n",
    "# in publication, 192 iterations are used, requiring < 30 minutes total\n",
    "random_search_iter = 192\n",
    "\n",
    "# number of iterations used in calculations of feature importance via permutation importance\n",
    "# increasing will decrease variance in feature imoprtance results but increase computation time\n",
    "# for publication 20 is used, taking approximately 10 minutes to run\n",
    "feature_importance_iter = 20\n",
    "\n",
    "# which metric to prioritize in model training\n",
    "# AUC evaluates threshold independent ability for a model to make decisions,\n",
    "# so it is a good metric to optimize before a model is deployed for \n",
    "# clinical settings.\n",
    "metric_to_prioritize = 'auc_roc_ovo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63e4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model setup (including parameter distributions for validation)\n",
    "model_names = [\"RandomForestClassifier\", \"KNeighborsClassifier\"]\n",
    "model_abbreviations = [\"RF\", \"kN\"]\n",
    "# Chat GPT helped to refine the parameter grid\n",
    "param_dists = [\n",
    "    {   # Random Forest                                                                         # options for...\n",
    "        \"classify\":                         [RandomForestClassifier(random_state=random_seed)], # model being trained (Random Forest)                                                             # options for...\n",
    "        \"reduce_dim__n_components\":         [5, 10, None],                                      # number of components to keep from PCA\n",
    "        \"classify__n_estimators\":           [100, 200, 300],                                    # number of decision trees in the random forest\n",
    "        \"classify__max_depth\":              [10, 20, 30, None],                                 # maximum allowed depth of the decision trees\n",
    "        \"classify__max_features\":           [\"sqrt\", \"log2\"],                                   # restriction on number of features considered per split\n",
    "        \"classify__bootstrap\":              [True],                                             # whether to bootstrap\n",
    "        \"classify__min_samples_split\":      [2, 3, 4],                                          # minimum number of samples required to split a node\n",
    "        \"classify__min_samples_leaf\":       [1, 2, 4]                                           # minimum number of samples required at a leaf\n",
    "    },\n",
    "    {   # K nearest neighbors                                                                   # options for...\n",
    "        \"classify\":                         [KNeighborsClassifier()],                           # model being trained (K nearest neighbors)\n",
    "        \"reduce_dim__n_components\":         [5, 10, None],                                      # number of components to keep from PCA\n",
    "        \"classify__weights\":                [\"uniform\", \"distance\"],                            # vote weighting method\n",
    "        \"classify__p\":                      [1, 1.5, 2, 2.25],                                  # exponent parameter for minkowski distance\n",
    "        \"classify__n_neighbors\":            [3, 5, 7, 11, 15, 19, 23, 25],                      # number of neighbors considered\n",
    "    }\n",
    "]\n",
    "model_options = dict(zip(model_names, param_dists))\n",
    "model_abbreviations = dict(zip(model_names, model_abbreviations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b4d4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "# extract data from file\n",
    "excel_file = \"../Data/Cell_Data.xlsx\"\n",
    "sheets_dict = pd.read_excel(excel_file, sheet_name = None) # dict where each value is a data frame (sheet)\n",
    "\n",
    "# cell type labels are sheet labels in the excel document and keys in the df dict\n",
    "# cell_types = ['L1', 'L2', 'L3', 'Monoblasts', 'Myeloblasts', 'Reactive Lymphs']\n",
    "cell_types = list(sheets_dict.keys())\n",
    "\n",
    "for cell_type in cell_types:\n",
    "    # add a cell type column to each data frame\n",
    "    sheets_dict[cell_type][\"cell_type\"] = cell_type\n",
    "\n",
    "# build one singular df with a class column identifying cell_type\n",
    "df_list = [sheets_dict[cell_type] for cell_type in cell_types]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# throw out name of image, area, and total image area\n",
    "combined_df.drop([\"Image\", \"Area (microns^2)\", \"TotalImageArea\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "815139c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lacunarity</th>\n",
       "      <th>Total Length (microns)</th>\n",
       "      <th>Endpoints</th>\n",
       "      <th>HGU (microns)</th>\n",
       "      <th>Branchpoints</th>\n",
       "      <th>Box-Counting Fractal Dimension</th>\n",
       "      <th>Curvature_50.0</th>\n",
       "      <th>% High Density Matrix</th>\n",
       "      <th>Alignment</th>\n",
       "      <th>Branchpoints/Total Length</th>\n",
       "      <th>Endpoints/Total Length</th>\n",
       "      <th>Average Fiber Length</th>\n",
       "      <th>Average Fiber Thickness</th>\n",
       "      <th>cell_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79.211</td>\n",
       "      <td>4005</td>\n",
       "      <td>69</td>\n",
       "      <td>58.043</td>\n",
       "      <td>367</td>\n",
       "      <td>1.175</td>\n",
       "      <td>26.812</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.11620</td>\n",
       "      <td>0.091635</td>\n",
       "      <td>0.017228</td>\n",
       "      <td>18.371560</td>\n",
       "      <td>14.656679</td>\n",
       "      <td>L1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110.888</td>\n",
       "      <td>3485</td>\n",
       "      <td>90</td>\n",
       "      <td>38.722</td>\n",
       "      <td>227</td>\n",
       "      <td>1.112</td>\n",
       "      <td>36.987</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.06317</td>\n",
       "      <td>0.065136</td>\n",
       "      <td>0.025825</td>\n",
       "      <td>21.987382</td>\n",
       "      <td>16.011478</td>\n",
       "      <td>L1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173.399</td>\n",
       "      <td>1880</td>\n",
       "      <td>43</td>\n",
       "      <td>43.721</td>\n",
       "      <td>59</td>\n",
       "      <td>1.024</td>\n",
       "      <td>30.577</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.09171</td>\n",
       "      <td>0.031383</td>\n",
       "      <td>0.022872</td>\n",
       "      <td>36.862745</td>\n",
       "      <td>28.829787</td>\n",
       "      <td>L1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96.193</td>\n",
       "      <td>3203</td>\n",
       "      <td>59</td>\n",
       "      <td>54.288</td>\n",
       "      <td>161</td>\n",
       "      <td>1.115</td>\n",
       "      <td>24.991</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.03657</td>\n",
       "      <td>0.050265</td>\n",
       "      <td>0.018420</td>\n",
       "      <td>29.118182</td>\n",
       "      <td>19.419294</td>\n",
       "      <td>L1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132.483</td>\n",
       "      <td>2414</td>\n",
       "      <td>54</td>\n",
       "      <td>44.704</td>\n",
       "      <td>82</td>\n",
       "      <td>1.088</td>\n",
       "      <td>26.482</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.05596</td>\n",
       "      <td>0.033969</td>\n",
       "      <td>0.022370</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>23.819387</td>\n",
       "      <td>L1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>86.239</td>\n",
       "      <td>3993</td>\n",
       "      <td>99</td>\n",
       "      <td>40.333</td>\n",
       "      <td>80</td>\n",
       "      <td>1.110</td>\n",
       "      <td>34.130</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.03727</td>\n",
       "      <td>0.020035</td>\n",
       "      <td>0.024793</td>\n",
       "      <td>44.614525</td>\n",
       "      <td>16.403706</td>\n",
       "      <td>Reactive Lymphs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>124.097</td>\n",
       "      <td>2777</td>\n",
       "      <td>61</td>\n",
       "      <td>45.525</td>\n",
       "      <td>88</td>\n",
       "      <td>1.097</td>\n",
       "      <td>39.728</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.10080</td>\n",
       "      <td>0.031689</td>\n",
       "      <td>0.021966</td>\n",
       "      <td>37.275168</td>\n",
       "      <td>19.877566</td>\n",
       "      <td>Reactive Lymphs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>76.126</td>\n",
       "      <td>3913</td>\n",
       "      <td>65</td>\n",
       "      <td>60.200</td>\n",
       "      <td>126</td>\n",
       "      <td>1.148</td>\n",
       "      <td>33.045</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.07569</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.016611</td>\n",
       "      <td>40.973822</td>\n",
       "      <td>19.039100</td>\n",
       "      <td>Reactive Lymphs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>75.398</td>\n",
       "      <td>4100</td>\n",
       "      <td>58</td>\n",
       "      <td>70.690</td>\n",
       "      <td>138</td>\n",
       "      <td>1.170</td>\n",
       "      <td>27.441</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.03438</td>\n",
       "      <td>0.033659</td>\n",
       "      <td>0.014146</td>\n",
       "      <td>41.836735</td>\n",
       "      <td>18.024390</td>\n",
       "      <td>Reactive Lymphs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>90.529</td>\n",
       "      <td>4475</td>\n",
       "      <td>56</td>\n",
       "      <td>79.911</td>\n",
       "      <td>102</td>\n",
       "      <td>1.153</td>\n",
       "      <td>37.669</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.01084</td>\n",
       "      <td>0.022793</td>\n",
       "      <td>0.012514</td>\n",
       "      <td>56.645570</td>\n",
       "      <td>14.569832</td>\n",
       "      <td>Reactive Lymphs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2600 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lacunarity  Total Length (microns)  Endpoints  HGU (microns)  \\\n",
       "0         79.211                    4005         69         58.043   \n",
       "1        110.888                    3485         90         38.722   \n",
       "2        173.399                    1880         43         43.721   \n",
       "3         96.193                    3203         59         54.288   \n",
       "4        132.483                    2414         54         44.704   \n",
       "...          ...                     ...        ...            ...   \n",
       "2595      86.239                    3993         99         40.333   \n",
       "2596     124.097                    2777         61         45.525   \n",
       "2597      76.126                    3913         65         60.200   \n",
       "2598      75.398                    4100         58         70.690   \n",
       "2599      90.529                    4475         56         79.911   \n",
       "\n",
       "      Branchpoints  Box-Counting Fractal Dimension  Curvature_50.0  \\\n",
       "0              367                           1.175          26.812   \n",
       "1              227                           1.112          36.987   \n",
       "2               59                           1.024          30.577   \n",
       "3              161                           1.115          24.991   \n",
       "4               82                           1.088          26.482   \n",
       "...            ...                             ...             ...   \n",
       "2595            80                           1.110          34.130   \n",
       "2596            88                           1.097          39.728   \n",
       "2597           126                           1.148          33.045   \n",
       "2598           138                           1.170          27.441   \n",
       "2599           102                           1.153          37.669   \n",
       "\n",
       "      % High Density Matrix  Alignment  Branchpoints/Total Length  \\\n",
       "0                     0.587    0.11620                   0.091635   \n",
       "1                     0.558    0.06317                   0.065136   \n",
       "2                     0.542    0.09171                   0.031383   \n",
       "3                     0.622    0.03657                   0.050265   \n",
       "4                     0.575    0.05596                   0.033969   \n",
       "...                     ...        ...                        ...   \n",
       "2595                  0.655    0.03727                   0.020035   \n",
       "2596                  0.552    0.10080                   0.031689   \n",
       "2597                  0.745    0.07569                   0.032200   \n",
       "2598                  0.739    0.03438                   0.033659   \n",
       "2599                  0.652    0.01084                   0.022793   \n",
       "\n",
       "      Endpoints/Total Length  Average Fiber Length  Average Fiber Thickness  \\\n",
       "0                   0.017228             18.371560                14.656679   \n",
       "1                   0.025825             21.987382                16.011478   \n",
       "2                   0.022872             36.862745                28.829787   \n",
       "3                   0.018420             29.118182                19.419294   \n",
       "4                   0.022370             35.500000                23.819387   \n",
       "...                      ...                   ...                      ...   \n",
       "2595                0.024793             44.614525                16.403706   \n",
       "2596                0.021966             37.275168                19.877566   \n",
       "2597                0.016611             40.973822                19.039100   \n",
       "2598                0.014146             41.836735                18.024390   \n",
       "2599                0.012514             56.645570                14.569832   \n",
       "\n",
       "            cell_type  \n",
       "0                  L1  \n",
       "1                  L1  \n",
       "2                  L1  \n",
       "3                  L1  \n",
       "4                  L1  \n",
       "...               ...  \n",
       "2595  Reactive Lymphs  \n",
       "2596  Reactive Lymphs  \n",
       "2597  Reactive Lymphs  \n",
       "2598  Reactive Lymphs  \n",
       "2599  Reactive Lymphs  \n",
       "\n",
       "[2600 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell types: {'Monoblasts', 'Myeloblasts', 'L1', 'L2', 'Reactive Lymphs', 'L3'}\n"
     ]
    }
   ],
   "source": [
    "### Summarize data\n",
    "# show dataframe\n",
    "display(combined_df)\n",
    "# show unique cell types\n",
    "print(\"Cell types:\", set(combined_df[\"cell_type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4633e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct the data permutations\n",
    "# now make different combinations of the data\n",
    "# L1 and RL\n",
    "l1_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"L1\", \"Reactive Lymphs\"])]\n",
    "# L2 and RL\n",
    "l2_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"L2\", \"Reactive Lymphs\"])]\n",
    "# L3 and RL\n",
    "l3_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"L3\", \"Reactive Lymphs\"])]\n",
    "# L1, L2, RL\n",
    "l1_and_l2_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"L1\", \"L2\", \"Reactive Lymphs\"])]\n",
    "# Monoblast and RL\n",
    "monoblast_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"Monoblasts\", \"Reactive Lymphs\"])]\n",
    "# Myeloblast and RL\n",
    "myeloblast_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"Myeloblasts\", \"Reactive Lymphs\"])]\n",
    "# Combined blasts and RL\n",
    "combined_blasts_and_rl_df = combined_df.copy()\n",
    "combined_blasts_and_rl_df.loc[combined_df[\"cell_type\"] != \"Reactive Lymphs\", \"cell_type\"] = \"Blasts\"\n",
    "\n",
    "# save dataframes in a dictionary with keys as their labels\n",
    "list_of_df = [combined_df, l1_and_rl_df, l2_and_rl_df, l3_and_rl_df, l1_and_l2_and_rl_df, monoblast_and_rl_df, myeloblast_and_rl_df, combined_blasts_and_rl_df]\n",
    "list_of_df_names = [\"All Cells\", \"L1 and RL\", \"L2 and RL\", \"L3 and RL\", \"L1 and L2 and RL\", \"Monoblast and RL\", \"Myeloblast and RL\", \"All blasts and RL\"]\n",
    "dict_of_df = dict(zip(list_of_df_names, list_of_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b1089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define pipeline\n",
    "# first, scale the data with a standard scaler\n",
    "# then, reduce dimensionality with PCA (number of dimensions tbd in model selection)\n",
    "# finally, use either random forest or knn to classify (also tbd)\n",
    "# help from: https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html#sphx-glr-auto-examples-compose-plot-compare-reduction-py\n",
    "\n",
    "# pass the selection of the classification algorithm through to validation\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaling\", StandardScaler()),\n",
    "        (\"reduce_dim\", PCA(random_state=random_seed)),\n",
    "        (\"classify\", \"passthrough\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "135d086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define custom model evaluation metrics\n",
    "def custom_success_metrics(y_test, y_pred, type):\n",
    "    # find confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # for each class, evaluate true positives, true negatives, etc (vectorized)\n",
    "    # Chat GPT helped here\n",
    "    tps = np.diag(cm)\n",
    "    fps = np.sum(cm, axis=0) - tps\n",
    "    fns = np.sum(cm, axis=1) - tps\n",
    "    tns = np.sum(cm) - (fps + fns + tps)\n",
    "    # each of these should be a vector with index corresponding to class\n",
    "    # now return a custom dictionary of balanced (macro-averaged) versions of accuracy, precision, sensitivity, specificity\n",
    "    # treat nan values as a score of 0, most likely are caused by empty classes\n",
    "    # also silence warnings caused by these issues during cv\n",
    "    # ChatGPT helped generate code to sanitize the final value and suppress warnings\n",
    "    val = None\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):            \n",
    "        match type:\n",
    "            case \"accuracy\": val = np.nanmean((tps + tns) / (tps + tns + fns + fps))\n",
    "            case \"precision\": val = np.nanmean(tps / (tps + fps))\n",
    "            case \"sensitivity\": val = np.nanmean(tps / (tps + fns))\n",
    "            case \"specificity\": val = np.nanmean(tns / (tns + fps))\n",
    "            case _: raise ValueError(f\"Type \\\"{type}\\\" is not supported\")\n",
    "    return 0.0 if np.isnan(val) else val\n",
    "\n",
    "# metrics to be calculated on each proposed model during cross validation (for model selection and hyperparameter optimization)\n",
    "metrics_dict = {\n",
    "    \"accuracy\": make_scorer(lambda ytest, ypred: custom_success_metrics(ytest, ypred, \"accuracy\")),\n",
    "    \"precision\": make_scorer(lambda ytest, ypred: custom_success_metrics(ytest, ypred, \"precision\")),\n",
    "    \"sensitivity\": make_scorer(lambda ytest, ypred: custom_success_metrics(ytest, ypred, \"sensitivity\")),\n",
    "    \"specificity\": make_scorer(lambda ytest, ypred: custom_success_metrics(ytest, ypred, \"specificity\")),\n",
    "    # use ovo and ovr for comparison\n",
    "    \"auc_roc_ovo\": make_scorer(roc_auc_score, multi_class = \"ovo\", response_method=[\"predict_proba\", \"decision_function\"]),\n",
    "    \"auc_roc_ovr\": make_scorer(roc_auc_score, multi_class = \"ovr\", response_method=[\"predict_proba\", \"decision_function\"]),\n",
    "    \"f1\": \"f1_macro\"\n",
    "}\n",
    "\n",
    "# Also define by class for use in final testing\n",
    "def custom_success_metrics_by_class(cm):\n",
    "    # uses confusion matrix\n",
    "    # for each class, evaluate true positives, true negatives, etc (vectorized)\n",
    "    # Chat GPT helped to get these straight\n",
    "    tps = np.diag(cm)\n",
    "    fps = np.sum(cm, axis=0) - tps\n",
    "    fns = np.sum(cm, axis=1) - tps\n",
    "    tns = np.sum(cm) - (fps + fns + tps)\n",
    "    # each of these should be a vector with index corresponding to class\n",
    "    # now compute the vector for either accuracy, precision, sensitivity or specificity\n",
    "    values = {\n",
    "        \"accuracy\"      : (tps + tns) / (tps + tns + fns + fps),\n",
    "        \"precision\"     : tps / (tps + fps),\n",
    "        \"sensitivity\"   : tps / (tps + fns),\n",
    "        \"specificity\"   : tns / (tns + fps)\n",
    "    }\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ba17956",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define replicable train-test split\n",
    "def ttsplit(df, random_state):\n",
    "    # split the data stratified by cell type 70-30 for training-testing\n",
    "    # use a fixed random state so that each split can be replicated exactly\n",
    "    # our response variable is cell type\n",
    "    y_data = df[\"cell_type\"]\n",
    "    X_data = df.drop(\"cell_type\", axis=1, inplace=False)\n",
    "    # split our data points into 70-30 training and testing sets, stratified by cell type\n",
    "    # returns X_train, X_test, y_train, y_test\n",
    "    return train_test_split(X_data, y_data, test_size = 0.3, random_state=random_state, shuffle=True, stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bbaa559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(X_train, y_train, param_dist):\n",
    "    # set up cross validation and randomized search\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "    # prioritize accuracy in training\n",
    "    best_model_indicator = 'rank_test_' + metric_to_prioritize\n",
    "    best_model_selector = lambda cv_results : np.argmin(cv_results[best_model_indicator])\n",
    "    search = RandomizedSearchCV(pipe, n_iter=random_search_iter, n_jobs=-1, param_distributions=param_dist, scoring=metrics_dict, refit=best_model_selector, cv=cv, verbose=False, error_score='raise', random_state=random_seed)\n",
    "    # cross validate\n",
    "    search.fit(X_train, y_train)\n",
    "    # save and return the validation results for this specific classification job\n",
    "    best_score_ind = np.argmin(search.cv_results_['mean_test_' + metric_to_prioritize])\n",
    "    best_score_mean = search.cv_results_['mean_test_' + metric_to_prioritize][best_score_ind]\n",
    "    best_score_std = search.cv_results_['std_test_' + metric_to_prioritize][best_score_ind]\n",
    "    results = pd.DataFrame(search.cv_results_)\n",
    "    results.sort_values(by=best_model_indicator, inplace=True)\n",
    "    validation_results = {\n",
    "        'results_df'                :   results,\n",
    "        'results'                   :   search.cv_results_,\n",
    "        'best_params'               :   search.best_params_,\n",
    "        'best_score_mean'           :   best_score_mean,\n",
    "        'best_score_std'            :   best_score_std,\n",
    "        'validation_set_X'          :   X_train,\n",
    "        'validation_set_y'          :   y_train,\n",
    "        'validation_n_iter'         :   random_search_iter,\n",
    "        'param_dist'                :   param_dist,\n",
    "    }\n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5ab251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### How to unpickle models\n",
    "# # (do this if you don't want to train models yourself)\n",
    "# with open(\"results.pickle\", \"rb\") as file:\n",
    "#     all_results = pickle.load(file)\n",
    "# # Check to make sure the filename is the right one.\n",
    "# # Comment out the cell below and run all after uncommenting this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba4b0e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Randomized Search CV on All Cells data with RandomForestClassifier:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Randomized Search CV on All blasts and RL data with KNeighborsClassifier: 100%|██████████| 16/16 [24:39<00:00, 92.48s/it]   \n"
     ]
    }
   ],
   "source": [
    "### Model validation and training\n",
    "\n",
    "# Perform random search for model selection and hyperparameter optimization\n",
    "# Run random search on the pipeline for each permutation of the dataset stored in dict_of_df\n",
    "# save results and info for each model in a dictionary\n",
    "all_results = dict()\n",
    "\n",
    "# create training jobs for each combination of classification task and model\n",
    "jobs = list()\n",
    "for name, df in dict_of_df.items():\n",
    "    for model_name in model_options.keys():\n",
    "        jobs.append((name, df, model_name))\n",
    "        all_results[(name, model_name)] = dict()\n",
    "        all_results[(name, model_name)]['dataframe'] = df\n",
    "\n",
    "# run all jobs with progress bar\n",
    "jobs = tqdm(jobs)\n",
    "for name, df, model_name in jobs:\n",
    "    # identify which permutation is being run\n",
    "    jobs.set_description(f\"Randomized Search CV on {name} data with {model_name}\")\n",
    "    # record results for this specific permutation in results_dict\n",
    "    results_dict = dict()\n",
    "    # split the data\n",
    "    X_train, X_test, y_train, y_test = ttsplit(df, random_seed)\n",
    "    # cross validate / randomized search\n",
    "    param_dist = model_options[model_name]\n",
    "    validation_results = model_validation(X_train, y_train, param_dist)\n",
    "    # save results\n",
    "    all_results[(name, model_name)].update({\n",
    "        'validation_results'    :   validation_results,\n",
    "        'train_test_split'      :   (X_train, X_test, y_train, y_test)\n",
    "    })\n",
    "\n",
    "# write the validation results dataframes to an excel sheet\n",
    "with pd.ExcelWriter(\"validation.xlsx\", engine='openpyxl') as writer:\n",
    "    for (name, model_name), results_dict in all_results.items():\n",
    "        results = results_dict['validation_results']['results_df']\n",
    "        results.to_excel(writer, sheet_name=name + \"_\" + model_abbreviations[model_name], index=False)\n",
    "\n",
    "# also pickle the all_results to get later if needed\n",
    "with open(\"results.pickle\", \"wb\") as file:\n",
    "    pickle.dump(all_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8dbcd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model testing and recording results\n",
    "# Test best performing models and report output in human readable format\n",
    "# Iterate through each group of labels for which a model was trained (all cells, L1 vs RL, etc)\n",
    "# Evaluate both k-nearest neighbors and random forest models on the final holdout / test set\n",
    "# Save information in results_<date>.txt and in results.pickle\n",
    "\n",
    "custom_metrics = ['accuracy', 'precision', 'sensitivity', 'specificity']\n",
    "# for feature importance\n",
    "metrics_dict_fi = {metric : make_scorer(lambda ytest, ypred: custom_success_metrics(ytest, ypred, metric)) for metric in custom_metrics}\n",
    "# these models will be trained with the best parameters selected in cross validation\n",
    "\n",
    "with open(\"results.txt\", \"w\") as file:\n",
    "    for (name, model_name), results_dict in all_results.items():\n",
    "        ### Train the model selected in validation (highest accuracy) on the entire validation set\n",
    "        # extract model parameters\n",
    "        model = model_options[model_name]['classify'][0]\n",
    "        best_estimator_params = results_dict['validation_results']['best_params']\n",
    "        best_estimator = Pipeline(\n",
    "            [\n",
    "                (\"scaling\", StandardScaler()),\n",
    "                (\"reduce_dim\", PCA()),\n",
    "                (\"classify\", clone(model))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # train the model\n",
    "        X_train, X_test, y_train, y_test = results_dict['train_test_split']\n",
    "        best_estimator.set_params(**best_estimator_params)\n",
    "        best_estimator.fit(X_train, y_train)\n",
    "\n",
    "        ### Test model on the resulting dataset\n",
    "        # get the best model found for the permutation given judging by accuracy\n",
    "        # find model predicted values and probabilities on the testing data\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        y_prob = best_estimator.predict_proba(X_test)\n",
    "        \n",
    "        ### Compute performance metrics\n",
    "        # compute the confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred, normalize=None)\n",
    "        \n",
    "        # evaluate and report model accuracy, precision, sensitivity, specificity\n",
    "        metric_vals_dict = custom_success_metrics_by_class(cm)\n",
    "        \n",
    "        # evaluate Area Under the Curve\n",
    "        # try both one-vs-one and one-vs-rest strategy for multi-class situations\n",
    "        if len(best_estimator.classes_) > 2:\n",
    "            # is multi-class classifier\n",
    "            ovo = roc_auc_score(y_test, y_prob, multi_class='ovo')\n",
    "            ovr = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "            metric_vals_dict['ROC AUC (OVO)'] = ovo\n",
    "            metric_vals_dict['ROC AUC (OVR)'] = ovr\n",
    "\n",
    "        else:\n",
    "            # is binary classifier\n",
    "            auc = roc_auc_score(y_test, y_prob[:,1])\n",
    "            metric_vals_dict['ROC AUC'] = auc\n",
    "\n",
    "        ### Compute feature importance (permutation-based importances)\n",
    "        # on training set\n",
    "        pi_dict_train = permutation_importance(best_estimator, X_train, y_train, scoring=metrics_dict_fi, n_repeats=feature_importance_iter, random_state=random_seed)\n",
    "        # on test set\n",
    "        pi_dict_test = permutation_importance(best_estimator, X_test, y_test, scoring=metrics_dict_fi, n_repeats=feature_importance_iter, random_state=random_seed)\n",
    "        # find averaged permutation-based feature importances\n",
    "        # final importance score for each feature is the balanced average of 8 values: the accuracy, precision, sensitivity and specificity permutation importances on train set and on test set\n",
    "        feature_vals = list()\n",
    "        for bunch in [pi_dict_train, pi_dict_test]:\n",
    "            for metric in custom_metrics:\n",
    "                feature_vals.append(bunch[metric]['importances_mean'])\n",
    "        feature_vals = np.stack(feature_vals)\n",
    "        # compute balanced average\n",
    "        feature_means = np.mean(feature_vals, axis=0)\n",
    "        # collect these means into a final feature importance dict\n",
    "        features = list(combined_df.columns)\n",
    "        feature_means_dict = dict(zip(features, feature_means))\n",
    "        # sort by final importance for output in the summary\n",
    "        feature_importance_list = sorted(feature_means_dict.items(), key=lambda k: feature_means_dict[k[0]], reverse=True)\n",
    "        \n",
    "        ### Write results summary to results.txt\n",
    "        file.write(f'*{name}*, {model_name}\\n')\n",
    "        file.write(f'classes:{str(best_estimator.classes_)}\\n')\n",
    "        file.write(f'confusion matrix:\\n{str(cm)}\\n')\n",
    "        for metric, met_vals in sorted(metric_vals_dict.items(), key=lambda x: x[0]):\n",
    "            if metric in custom_metrics:\n",
    "                file.write(f'{metric} by class: {str(met_vals)}\\n')\n",
    "                file.write(f'average {metric} (balanced, by class): {str(np.mean(met_vals))}\\n')\n",
    "            else:\n",
    "                file.write(f'{metric}: {str(met_vals)}\\n')\n",
    "        file.write(f\"Features: {features}\\n\")\n",
    "        file.write(\"Features in order of final importance:\\n\")\n",
    "        for i, pair in enumerate(feature_importance_list):\n",
    "            feature, importance = pair\n",
    "            dot = \".\"\n",
    "            file.write(f\"\\t{str(i+1)+dot:<5} {feature:<35} {importance}\\n\")\n",
    "        file.write(\"Model details:\\n\")\n",
    "        file.write(str(results_dict['validation_results']['best_params']) + '\\n')\n",
    "        file.write('\\n\\n')\n",
    "\n",
    "        ### Add results to results_dict dictionary (located within all_results) to pickle later\n",
    "        results_dict[\"final_results\"] = {\n",
    "            'trained_model'                         :   best_estimator,\n",
    "            'model_params'                          :   best_estimator_params,\n",
    "            'y_true'                                :   y_test,\n",
    "            'y_predicted'                           :   y_pred,\n",
    "            'y_predicted_probabilities'             :   y_prob,\n",
    "            'confusion_matrix'                      :   cm,\n",
    "            'metric_values'                         :   metric_vals_dict,\n",
    "            'classes'                               :   best_estimator.classes_,\n",
    "            'permutation_importance_n_repeats'      :   feature_importance_iter,\n",
    "            'permutation_importance_train'          :   pi_dict_train,\n",
    "            'permutation_importance_test'           :   pi_dict_test,\n",
    "            'feature_importance_final_importances'  :   feature_means_dict\n",
    "        }\n",
    "\n",
    "### Write the pickled all_results to get them later if needed\n",
    "with open(\"results.pickle\", \"wb\") as file:\n",
    "    pickle.dump(all_results, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
