{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba26fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from warnings import filterwarnings\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, make_scorer\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter warnings from numpy that show up occasionally during cv, should not affect performance\n",
    "filterwarnings(\"ignore\", message=\"invalid value encountered in cast\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cb34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters\n",
    "# random seed\n",
    "# allows for reproducibility\n",
    "# default is 42, which is used in publication\n",
    "random_seed = 42\n",
    "\n",
    "# number of iterations of random search to use to optimize each model's hyperparameters in validation\n",
    "# increasing will improve results but increase computation time\n",
    "# requires under 3 minutes for every 20 iterations with the following system specs:\n",
    "# Processor\tIntel(R) Core(TM) i7-8650U CPU @ 1.90GHz, 2112 Mhz, 4 Core(s), 8 Logical Processor(s)\n",
    "# Installed Physical Memory (RAM)\t16.0 GB\n",
    "# in publication, 192 iterations are used, requiring < 30 minutes total\n",
    "random_search_iter = 192\n",
    "\n",
    "# number of iterations used in calculations of feature importance via permutation importance\n",
    "# increasing will decrease variance in feature imoprtance results but increase computation time\n",
    "# for publication 20 is used, taking approximately 10 minutes to run\n",
    "feature_importance_iter = 20\n",
    "\n",
    "# which metric to prioritize in model training\n",
    "# AUC evaluates threshold independent ability for a model to make decisions,\n",
    "# so it is a good metric to optimize before a model is deployed for \n",
    "# clinical settings.\n",
    "metric_to_prioritize = 'auc_roc_ovo'\n",
    "\n",
    "# whether to only retrain particular models\n",
    "# useful in validation, etc if you only want to tweak something\n",
    "# about one model and retrain while not eliminating\n",
    "# the trained models in results.pickle\n",
    "use_previous_results = False\n",
    "# use_previous_results = True\n",
    "# specify which results to reuse below\n",
    "# results_to_use = []\n",
    "# results_to_use = [\"All Cells\", \"L1 and RL\", \"L2 and RL\", \"L3 and RL\", \"L1 and L2 and RL\", \"Monoblast and RL\", \"Myeloblast and RL\", \"All blasts and RL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model setup (including parameter distributions for validation)\n",
    "model_names = [\"RandomForestClassifier\", \"KNeighborsClassifier\"]\n",
    "model_abbreviations = [\"RF\", \"kN\"]\n",
    "# Chat GPT helped to refine the parameter grid\n",
    "param_dists = [\n",
    "    {   # Random Forest                                                                         # options for...\n",
    "        \"classify\":                         [RandomForestClassifier(random_state=random_seed)], # model being trained (Random Forest)                                                             # options for...\n",
    "        \"reduce_dim__n_components\":         [5, 10, None],                                      # number of components to keep from PCA\n",
    "        \"classify__n_estimators\":           [100, 200, 300],                                    # number of decision trees in the random forest\n",
    "        \"classify__max_depth\":              [10, 20, 30, None],                                 # maximum allowed depth of the decision trees\n",
    "        \"classify__max_features\":           [\"sqrt\", \"log2\"],                                   # restriction on number of features considered per split\n",
    "        \"classify__bootstrap\":              [True],                                             # whether to bootstrap\n",
    "        \"classify__min_samples_split\":      [2, 3, 4],                                          # minimum number of samples required to split a node\n",
    "        \"classify__min_samples_leaf\":       [1, 2, 4]                                           # minimum number of samples required at a leaf\n",
    "    },\n",
    "    {   # K nearest neighbors                                                                   # options for...\n",
    "        \"classify\":                         [KNeighborsClassifier()],                           # model being trained (K nearest neighbors)\n",
    "        \"reduce_dim__n_components\":         [5, 10, None],                                      # number of components to keep from PCA\n",
    "        \"classify__weights\":                [\"uniform\", \"distance\"],                            # vote weighting method\n",
    "        \"classify__p\":                      [1, 1.5, 2, 2.25],                                  # exponent parameter for minkowski distance\n",
    "        \"classify__n_neighbors\":            [3, 5, 7, 11, 15, 19, 23, 25],                      # number of neighbors considered\n",
    "    }\n",
    "]\n",
    "model_options = dict(zip(model_names, param_dists))\n",
    "model_abbreviations = dict(zip(model_names, model_abbreviations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "# extract data from file\n",
    "excel_file = \"../Data/Cell_Data.xlsx\"\n",
    "sheets_dict = pd.read_excel(excel_file, sheet_name = None) # dict where each value is a data frame (sheet)\n",
    "\n",
    "# cell type labels are sheet labels in the excel document and keys in the df dict\n",
    "# cell_types = ['L1', 'L2', 'L3', 'Monoblasts', 'Myeloblasts', 'Reactive Lymphs']\n",
    "cell_types = list(sheets_dict.keys())\n",
    "\n",
    "for cell_type in cell_types:\n",
    "    # add a cell type column to each data frame\n",
    "    sheets_dict[cell_type][\"cell_type\"] = cell_type\n",
    "\n",
    "# build one singular df with a class column identifying cell_type\n",
    "df_list = [sheets_dict[cell_type] for cell_type in cell_types]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# throw out name of image, area, and total image area\n",
    "combined_df.drop([\"Image\", \"Area (microns^2)\", \"TotalImageArea\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815139c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Summarize data\n",
    "# show dataframe\n",
    "display(combined_df)\n",
    "# show unique cell types\n",
    "print(\"Cell types:\", set(combined_df[\"cell_type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4633e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct the data permutations\n",
    "# now make different combinations of the data\n",
    "# --- approximately balanced datasets ---\n",
    "# L1 and RL\n",
    "l1_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"L1\", \"Reactive Lymphs\"])]\n",
    "# L2 and RL\n",
    "l2_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"L2\", \"Reactive Lymphs\"])]\n",
    "# L3 and RL\n",
    "l3_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"L3\", \"Reactive Lymphs\"])]\n",
    "# L1, L2, RL\n",
    "l1_and_l2_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"L1\", \"L2\", \"Reactive Lymphs\"])]\n",
    "# Monoblast and RL\n",
    "monoblast_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"Monoblasts\", \"Reactive Lymphs\"])]\n",
    "# Myeloblast and RL\n",
    "myeloblast_and_rl_df = combined_df[combined_df[\"cell_type\"].isin([\"Myeloblasts\", \"Reactive Lymphs\"])]\n",
    "\n",
    "# --- imbalanced datasets ---\n",
    "# Combined blasts and RL\n",
    "# downsample the blasts to avoid class imbalance\n",
    "# get all reactive lymphs as a dataframe\n",
    "rls = combined_df.loc[combined_df[\"cell_type\"] == \"Reactive Lymphs\"]\n",
    "# get enough blasts from each category to approximately match the number of RLs\n",
    "num_rls = len(rls)\n",
    "blast_names = [\"L1\", \"L2\", \"L3\", \"Monoblasts\", \"Myeloblasts\"]\n",
    "num_blast_per_category = num_rls//len(blast_names)\n",
    "downsampled_blast_frames = list()\n",
    "for blast_name in blast_names:\n",
    "    # randomly downsample from each cell type\n",
    "    downsampled_blast_frames.append(combined_df.loc[combined_df[\"cell_type\"] == blast_name].sample(num_blast_per_category, random_state=random_seed))\n",
    "\n",
    "downsampled_blasts = pd.concat(downsampled_blast_frames)\n",
    "downsampled_blasts[\"cell_type\"] = \"Blasts\"\n",
    "combined_blasts_and_rl_df = pd.concat([rls, downsampled_blasts])\n",
    "\n",
    "# save dataframes in a dictionary with keys as their labels\n",
    "list_of_df = [combined_df, l1_and_rl_df, l2_and_rl_df, l3_and_rl_df, l1_and_l2_and_rl_df, monoblast_and_rl_df, myeloblast_and_rl_df, combined_blasts_and_rl_df]\n",
    "list_of_df_names = [\"All Cells\", \"L1 and RL\", \"L2 and RL\", \"L3 and RL\", \"L1 and L2 and RL\", \"Monoblast and RL\", \"Myeloblast and RL\", \"All blasts and RL\"]\n",
    "dict_of_df = dict(zip(list_of_df_names, list_of_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define pipeline\n",
    "# first, scale the data with a standard scaler\n",
    "# then, reduce dimensionality with PCA (number of dimensions tbd in model selection)\n",
    "# finally, use either random forest or knn to classify (also tbd)\n",
    "# help from: https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html#sphx-glr-auto-examples-compose-plot-compare-reduction-py\n",
    "\n",
    "# pass the selection of the classification algorithm through to validation\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaling\", StandardScaler()),\n",
    "        (\"reduce_dim\", PCA(random_state=random_seed)),\n",
    "        (\"classify\", \"passthrough\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define custom model evaluation metrics\n",
    "def custom_success_metrics(y_test, y_pred, type):\n",
    "    # find confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # for each class, evaluate true positives, true negatives, etc (vectorized)\n",
    "    # Chat GPT helped here\n",
    "    tps = np.diag(cm)\n",
    "    fps = np.sum(cm, axis=0) - tps\n",
    "    fns = np.sum(cm, axis=1) - tps\n",
    "    tns = np.sum(cm) - (fps + fns + tps)\n",
    "    # each of these should be a vector with index corresponding to class\n",
    "    # now return a custom dictionary of balanced (macro-averaged) versions of accuracy, precision, sensitivity, specificity\n",
    "    # treat nan values as a score of 0, most likely are caused by empty classes\n",
    "    # also silence warnings caused by these issues during cv\n",
    "    # ChatGPT helped generate code to sanitize the final value and suppress warnings\n",
    "    val = None\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):            \n",
    "        match type:\n",
    "            case \"accuracy\": val = np.nanmean((tps + tns) / (tps + tns + fns + fps))\n",
    "            case \"precision\": val = np.nanmean(tps / (tps + fps))\n",
    "            case \"sensitivity\": val = np.nanmean(tps / (tps + fns))\n",
    "            case \"specificity\": val = np.nanmean(tns / (tns + fps))\n",
    "            case _: raise ValueError(f\"Type \\\"{type}\\\" is not supported\")\n",
    "    return 0.0 if np.isnan(val) else val\n",
    "\n",
    "# metrics to be calculated on each proposed model during cross validation (for model selection and hyperparameter optimization)\n",
    "metrics_dict = {\n",
    "    \"accuracy\": make_scorer(lambda ytest, ypred: custom_success_metrics(ytest, ypred, \"accuracy\")),\n",
    "    \"precision\": make_scorer(lambda ytest, ypred: custom_success_metrics(ytest, ypred, \"precision\")),\n",
    "    \"sensitivity\": make_scorer(lambda ytest, ypred: custom_success_metrics(ytest, ypred, \"sensitivity\")),\n",
    "    \"specificity\": make_scorer(lambda ytest, ypred: custom_success_metrics(ytest, ypred, \"specificity\")),\n",
    "    # use ovo and ovr for comparison\n",
    "    \"auc_roc_ovo\": make_scorer(roc_auc_score, multi_class = \"ovo\", response_method=[\"predict_proba\", \"decision_function\"]),\n",
    "    \"auc_roc_ovr\": make_scorer(roc_auc_score, multi_class = \"ovr\", response_method=[\"predict_proba\", \"decision_function\"]),\n",
    "    \"f1\": \"f1_macro\"\n",
    "}\n",
    "\n",
    "# Also define by class for use in final testing\n",
    "def custom_success_metrics_by_class(cm):\n",
    "    # uses confusion matrix\n",
    "    # for each class, evaluate true positives, true negatives, etc (vectorized)\n",
    "    # Chat GPT helped to get these straight\n",
    "    tps = np.diag(cm)\n",
    "    fps = np.sum(cm, axis=0) - tps\n",
    "    fns = np.sum(cm, axis=1) - tps\n",
    "    tns = np.sum(cm) - (fps + fns + tps)\n",
    "    # each of these should be a vector with index corresponding to class\n",
    "    # now compute the vector for either accuracy, precision, sensitivity or specificity\n",
    "    values = {\n",
    "        \"accuracy\"      : (tps + tns) / (tps + tns + fns + fps),\n",
    "        \"precision\"     : tps / (tps + fps),\n",
    "        \"sensitivity\"   : tps / (tps + fns),\n",
    "        \"specificity\"   : tns / (tns + fps)\n",
    "    }\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba17956",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define replicable train-test split\n",
    "def ttsplit(df, random_state):\n",
    "    # split the data stratified by cell type 70-30 for training-testing\n",
    "    # use a fixed random state so that each split can be replicated exactly\n",
    "    # our response variable is cell type\n",
    "    y_data = df[\"cell_type\"]\n",
    "    X_data = df.drop(\"cell_type\", axis=1, inplace=False)\n",
    "    # split our data points into 70-30 training and testing sets, stratified by cell type\n",
    "    # returns X_train, X_test, y_train, y_test\n",
    "    return train_test_split(X_data, y_data, test_size = 0.3, random_state=random_state, shuffle=True, stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbaa559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(X_train, y_train, param_dist):\n",
    "    # set up cross validation and randomized search\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "    # prioritize accuracy in training\n",
    "    best_model_indicator = 'rank_test_' + metric_to_prioritize\n",
    "    best_model_selector = lambda cv_results : np.argmin(cv_results[best_model_indicator])\n",
    "    search = RandomizedSearchCV(pipe, n_iter=random_search_iter, n_jobs=-1, param_distributions=param_dist, scoring=metrics_dict, refit=best_model_selector, cv=cv, verbose=False, error_score='raise', random_state=random_seed)\n",
    "    # cross validate\n",
    "    search.fit(X_train, y_train)\n",
    "    # save and return the validation results for this specific classification job\n",
    "    best_score_ind = np.argmin(search.cv_results_['mean_test_' + metric_to_prioritize])\n",
    "    best_score_mean = search.cv_results_['mean_test_' + metric_to_prioritize][best_score_ind]\n",
    "    best_score_std = search.cv_results_['std_test_' + metric_to_prioritize][best_score_ind]\n",
    "    results = pd.DataFrame(search.cv_results_)\n",
    "    results.sort_values(by=best_model_indicator, inplace=True)\n",
    "    validation_results = {\n",
    "        'results_df'                :   results,\n",
    "        'results'                   :   search.cv_results_,\n",
    "        'best_params'               :   search.best_params_,\n",
    "        'best_score_mean'           :   best_score_mean,\n",
    "        'best_score_std'            :   best_score_std,\n",
    "        'validation_set_X'          :   X_train,\n",
    "        'validation_set_y'          :   y_train,\n",
    "        'validation_n_iter'         :   random_search_iter,\n",
    "        'param_dist'                :   param_dist,\n",
    "    }\n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model validation and training\n",
    "\n",
    "# Perform random search for model selection and hyperparameter optimization\n",
    "# Run random search on the pipeline for each permutation of the dataset stored in dict_of_df\n",
    "# save results and info for each model in a dictionary\n",
    "if use_previous_results:\n",
    "    with open(\"results.pickle\", \"rb\") as file:\n",
    "        all_results = pickle.load(file)\n",
    "else:\n",
    "    all_results = dict()\n",
    "\n",
    "# create training jobs for each combination of classification task and model\n",
    "jobs = list()\n",
    "for name, df in dict_of_df.items():\n",
    "    for model_name in model_options.keys():\n",
    "        if (name not in results_to_use) or (use_previous_results == False):\n",
    "            jobs.append((name, df, model_name))\n",
    "            all_results[(name, model_name)] = dict()\n",
    "            all_results[(name, model_name)]['dataframe'] = df\n",
    "\n",
    "# run all jobs with progress bar\n",
    "jobs = tqdm(jobs)\n",
    "for name, df, model_name in jobs:\n",
    "    # identify which permutation is being run\n",
    "    jobs.set_description(f\"Randomized Search CV on {name} data with {model_name}\")\n",
    "    # record results for this specific permutation in results_dict\n",
    "    results_dict = dict()\n",
    "    # split the data\n",
    "    X_train, X_test, y_train, y_test = ttsplit(df, random_seed)\n",
    "    # cross validate / randomized search\n",
    "    param_dist = model_options[model_name]\n",
    "    validation_results = model_validation(X_train, y_train, param_dist)\n",
    "    # save results\n",
    "    all_results[(name, model_name)].update({\n",
    "        'validation_results'    :   validation_results,\n",
    "        'train_test_split'      :   (X_train, X_test, y_train, y_test)\n",
    "    })\n",
    "    # since we have created / updated the model, there should be no testing results\n",
    "    # remove this key if it exists (do nothing if not)\n",
    "    all_results[(name, model_name)].pop('final_results', None)\n",
    "\n",
    "# write the validation results dataframes to an excel sheet\n",
    "with pd.ExcelWriter(\"validation.xlsx\", engine='openpyxl') as writer:\n",
    "    for (name, model_name), results_dict in all_results.items():\n",
    "        results = results_dict['validation_results']['results_df']\n",
    "        results.to_excel(writer, sheet_name=name + \"_\" + model_abbreviations[model_name], index=False)\n",
    "\n",
    "# also pickle the all_results to get later if needed\n",
    "with open(\"results.pickle\", \"wb\") as file:\n",
    "    pickle.dump(all_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dbcd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model testing and recording results\n",
    "# Test best performing models and report output in human readable format\n",
    "# Iterate through each group of labels for which a model was trained (all cells, L1 vs RL, etc)\n",
    "# Evaluate both k-nearest neighbors and random forest models on the final holdout / test set\n",
    "# Save information in results_<date>.txt and in results.pickle\n",
    "\n",
    "custom_metrics = ['accuracy', 'precision', 'sensitivity', 'specificity']\n",
    "# for feature importance\n",
    "metrics_dict_fi = {metric : make_scorer(lambda ytest, ypred: custom_success_metrics(ytest, ypred, metric)) for metric in custom_metrics}\n",
    "# these models will be trained with the best parameters selected in cross validation\n",
    "\n",
    "with open(\"results.txt\", \"w\") as file:\n",
    "    for (name, model_name), results_dict in all_results.items():\n",
    "        ### Train the model selected in validation (highest accuracy) on the entire validation set\n",
    "        # extract model parameters\n",
    "        model = model_options[model_name]['classify'][0]\n",
    "        best_estimator_params = results_dict['validation_results']['best_params']\n",
    "        best_estimator = Pipeline(\n",
    "            [\n",
    "                (\"scaling\", StandardScaler()),\n",
    "                (\"reduce_dim\", PCA()),\n",
    "                (\"classify\", clone(model))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # train the model\n",
    "        X_train, X_test, y_train, y_test = results_dict['train_test_split']\n",
    "        best_estimator.set_params(**best_estimator_params)\n",
    "        best_estimator.fit(X_train, y_train)\n",
    "\n",
    "        ### Test model on the resulting dataset\n",
    "        # get the best model found for the permutation given judging by accuracy\n",
    "        # find model predicted values and probabilities on the testing data\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        y_prob = best_estimator.predict_proba(X_test)\n",
    "        \n",
    "        ### Compute performance metrics\n",
    "        # compute the confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred, normalize=None)\n",
    "        \n",
    "        # evaluate and report model accuracy, precision, sensitivity, specificity\n",
    "        metric_vals_dict = custom_success_metrics_by_class(cm)\n",
    "        \n",
    "        # evaluate Area Under the Curve\n",
    "        # try both one-vs-one and one-vs-rest strategy for multi-class situations\n",
    "        if len(best_estimator.classes_) > 2:\n",
    "            # is multi-class classifier\n",
    "            ovo = roc_auc_score(y_test, y_prob, multi_class='ovo')\n",
    "            ovr = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "            metric_vals_dict['ROC AUC (OVO)'] = ovo\n",
    "            metric_vals_dict['ROC AUC (OVR)'] = ovr\n",
    "\n",
    "        else:\n",
    "            # is binary classifier\n",
    "            auc = roc_auc_score(y_test, y_prob[:,1])\n",
    "            metric_vals_dict['ROC AUC'] = auc\n",
    "\n",
    "        ### Compute feature importance (permutation-based importances)\n",
    "        # on training set\n",
    "        pi_dict_train = permutation_importance(best_estimator, X_train, y_train, scoring=metrics_dict_fi, n_repeats=feature_importance_iter, random_state=random_seed)\n",
    "        # on test set\n",
    "        pi_dict_test = permutation_importance(best_estimator, X_test, y_test, scoring=metrics_dict_fi, n_repeats=feature_importance_iter, random_state=random_seed)\n",
    "        # find averaged permutation-based feature importances\n",
    "        # final importance score for each feature is the balanced average of 8 values: the accuracy, precision, sensitivity and specificity permutation importances on train set and on test set\n",
    "        feature_vals = list()\n",
    "        for bunch in [pi_dict_train, pi_dict_test]:\n",
    "            for metric in custom_metrics:\n",
    "                feature_vals.append(bunch[metric]['importances_mean'])\n",
    "        feature_vals = np.stack(feature_vals)\n",
    "        # compute balanced average\n",
    "        feature_means = np.mean(feature_vals, axis=0)\n",
    "        # collect these means into a final feature importance dict\n",
    "        features = list(combined_df.columns)\n",
    "        feature_means_dict = dict(zip(features, feature_means))\n",
    "        # sort by final importance for output in the summary\n",
    "        feature_importance_list = sorted(feature_means_dict.items(), key=lambda k: feature_means_dict[k[0]], reverse=True)\n",
    "        \n",
    "        ### Write results summary to results.txt\n",
    "        file.write(f'*{name}*, {model_name}\\n')\n",
    "        file.write(f'classes:{str(best_estimator.classes_)}\\n')\n",
    "        file.write(f'confusion matrix:\\n{str(cm)}\\n')\n",
    "        for metric, met_vals in sorted(metric_vals_dict.items(), key=lambda x: x[0]):\n",
    "            if metric in custom_metrics:\n",
    "                file.write(f'{metric} by class: {str(met_vals)}\\n')\n",
    "                file.write(f'average {metric} (balanced, by class): {str(np.mean(met_vals))}\\n')\n",
    "            else:\n",
    "                file.write(f'{metric}: {str(met_vals)}\\n')\n",
    "        file.write(f\"Features: {features}\\n\")\n",
    "        file.write(\"Features in order of final importance:\\n\")\n",
    "        for i, pair in enumerate(feature_importance_list):\n",
    "            feature, importance = pair\n",
    "            dot = \".\"\n",
    "            file.write(f\"\\t{str(i+1)+dot:<5} {feature:<35} {importance}\\n\")\n",
    "        file.write(\"Model details:\\n\")\n",
    "        file.write(str(results_dict['validation_results']['best_params']) + '\\n')\n",
    "        file.write('\\n\\n')\n",
    "\n",
    "        ### Add results to results_dict dictionary (located within all_results) to pickle later\n",
    "        results_dict[\"final_results\"] = {\n",
    "            'trained_model'                         :   best_estimator,\n",
    "            'model_params'                          :   best_estimator_params,\n",
    "            'y_true'                                :   y_test,\n",
    "            'y_predicted'                           :   y_pred,\n",
    "            'y_predicted_probabilities'             :   y_prob,\n",
    "            'confusion_matrix'                      :   cm,\n",
    "            'metric_values'                         :   metric_vals_dict,\n",
    "            'classes'                               :   best_estimator.classes_,\n",
    "            'permutation_importance_n_repeats'      :   feature_importance_iter,\n",
    "            'permutation_importance_train'          :   pi_dict_train,\n",
    "            'permutation_importance_test'           :   pi_dict_test,\n",
    "            'feature_importance_final_importances'  :   feature_means_dict\n",
    "        }\n",
    "\n",
    "### Write the pickled all_results to get them later if needed\n",
    "with open(\"results.pickle\", \"wb\") as file:\n",
    "    pickle.dump(all_results, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
